# ProblemSet1
Data 340: Fall 2023

For Problem Set 1:
Naive Bayes Model
-After extracting all of the files from Kaggle, I was able to take some excerpts 
 from class notebooks to tokenize words or phrases using different tokenizers
 (spacy, Genshim, and NLTK).
- I was then able to move onto the Naive Bayes Model. From looking at the files
  first few columns, there seems to be a disproportionate amount of reviews
  with two's. As expected, my confusion matrix showed the most density
  in the two's for sentiment. My model performance, however, was around
  59% performance (approximately). I will have to keep training it to get
  better accuracy and performance.

Logistic Regression Model
-I tried to follow similar logic used in class by using sklearn to output 
 performance. My logistic regression model performed so much worse than my
 Naive Bayes model, but still showed the most clustering/density in the two's.
